---
title: "project code"
author: "Rob McNeil and Jessica Chaffee"
date: "2024-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load library, import data set

```{r, echo=FALSE}
library(caret)
library(randomForest)
library(ggplot2)
library(ggcorrplot)
library(knitr)
library(dplyr)
library(tidyr)             
library(glmnet)
library(kableExtra)
library(elasticnet)
library(knitr)

#linux pathway
#adpkd <- read.csv("/home/robmcneil/Documents/advanced_data/Project3_data.csv")
adpkd <- read.csv("~/Downloads/Project3_data.csv")
#windows pathway 
#adpkd <-read.csv("C:\Users\rrm10\OneDrive\Desktop\advanced data analysis\Project3_data.csv")


```

```{r}
#summary of missing data (Found no missing data)
colSums(is.na(adpkd))
```

```{r}
# Visualize data via histograms of numeric variables
numeric_columns <- sapply(adpkd, is.numeric)

for (col in names(adpkd)[numeric_columns]) {
  hist(adpkd[[col]], main = paste("Histogram of", col), xlab = col)
}
```
```{r}

#code for table 1 
demographic_summary <- gum %>% #Use gum dataset to see summary table for all 130 participants, use gum2 data set for the summary statistics of just the participants in the final analysis
  group_by(trtgroup) %>%
  summarise(
    `Male (%)` = as.character(round(sum(gender == 1, na.rm = TRUE) / n() * 100, 2)),
    `Female (%)` = as.character(round(sum(gender == 2, na.rm = TRUE) / n() * 100, 2)),
    `Native American (%)` = as.character(round(sum(race == 1, na.rm = TRUE) / n() * 100, 2)),
    `African American (%)` = as.character(round(sum(race == 2, na.rm = TRUE) / n() * 100, 2)),
    `Asian (%)` = as.character(round(sum(race == 4, na.rm = TRUE) / n() * 100, 2)),
    `White (%)` = as.character(round(sum(race == 5, na.rm = TRUE) / n() * 100, 2)),
    `Age (Mean(SD))` = as.character(paste0(round(mean(age, na.rm = TRUE), 2), " (", round(sd(age, na.rm = TRUE), 2), ")")),
    `Smoker (%)` = as.character(round(sum(smoker == 1, na.rm = TRUE) / n() * 100, 2)),
    `Initial Attachment Loss (Mean(SD))` = as.character(paste0(round(mean(attachbase, na.rm = TRUE), 2), " (", round(sd(attachbase, na.rm = TRUE), 2), ")")),
    `Initial Pocket Depth (Mean(SD))` = as.character(paste0(round(mean(pdbase, na.rm = TRUE), 2), " (", round(sd(pdbase, na.rm = TRUE), 2), ")")),
    `Attachment Loss After 1 Year (Mean(SD))` = as.character(paste0(round(mean(attach1year, na.rm = TRUE), 2), " (", round(sd(attach1year, na.rm = TRUE), 2), ")")),
    `Pocket Depth After 1 Year (Mean(SD))` = as.character(paste0(round(mean(pd1year, na.rm = TRUE), 2), " (", round(sd(pd1year, na.rm = TRUE), 2), ")"))
  ) %>%
  ungroup() %>%
  mutate(trtgroup = case_when(
    trtgroup == 1 ~ "Placebo (group 1)",
    trtgroup == 2 ~ "No Treatment (group 2)",
    trtgroup == 3 ~ "Low Concentration (group 3)",
    trtgroup == 4 ~ "Medium Concentration (group 4)",
    trtgroup == 5 ~ "High Concentration (group 5)"
  )) %>%
  pivot_longer(cols = -trtgroup, names_to = "Demographic", values_to = "Value") %>%
  pivot_wider(names_from = trtgroup, values_from = Value)

# Print the summary table
kable(demographic_summary, caption = "Table 1: Demographic and Other Characteristic Information by Treatment Group")

```


```{r}
# Print the summary table 1 with lines between rows
kable(demographic_summary, caption = "Table 1: Demographic and Other Characteristic Information by Treatment Group") %>%
  kable_styling() %>%
  row_spec(0, bold = TRUE) %>% # Bold the header
  row_spec(1:nrow(demographic_summary), extra_css = "border-top: 1px solid black; border-bottom: 1px solid black;")

```

```{r}
# Create table 1 (features stratified by progression)

summary_table <- adpkd %>%
  summarise(
    'Geometry Feature 1 (Mean(SD))' = as.character(paste0(formatC(mean(geom1, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(geom1, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Geometry Feature 2 (Mean(SD))' = as.character(paste0(formatC(mean(geom2, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(geom2, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Gabor Transformation 1 (Mean(SD))' = as.character(paste0(formatC(mean(gabor1, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(gabor1, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Gabor Transformation 2 (Mean(SD))' = as.character(paste0(formatC(mean(gabor2, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(gabor2, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Gabor Transformation 3 (Mean(SD))' = as.character(paste0(formatC(mean(gabor3, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(gabor3, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Gabor Transformation 4 (Mean(SD))' = as.character(paste0(formatC(mean(gabor4, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(gabor4, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Gabor Transformation 5 (Mean(SD))' = as.character(paste0(formatC(mean(gabor5, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(gabor5, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Gray Level Co-Occurrence Matrix 1 (Mean(SD))' = as.character(paste0(formatC(mean(glcm1, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(glcm1, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Gray Level Co-Occurrence Matrix 2 (Mean(SD))' = as.character(paste0(formatC(mean(glcm2, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(glcm2, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Image Textures 1 (Mean(SD))' = as.character(paste0(formatC(mean(txti1, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(txti1, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Image Textures 2 (Mean(SD))' = as.character(paste0(formatC(mean(txti2, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(txti2, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Image Textures 3 (Mean(SD))' = as.character(paste0(formatC(mean(txti3, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(txti3, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Image Textures 4 (Mean(SD))' = as.character(paste0(formatC(mean(txti4, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(txti4, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Image Textures 5 (Mean(SD))' = as.character(paste0(formatC(mean(txti5, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(txti5, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Local Binary Pattern 1 (Mean(SD))' = as.character(paste0(formatC(mean(lbp1, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(lbp1, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Local Binary Pattern 2 (Mean(SD))' = as.character(paste0(formatC(mean(lbp2, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(lbp2, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Local Binary Pattern 3 (Mean(SD))' = as.character(paste0(formatC(mean(lbp3, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(lbp3, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Local Binary Pattern 4 (Mean(SD))' = as.character(paste0(formatC(mean(lbp4, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(lbp4, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Local Binary Pattern 5 (Mean(SD))' = as.character(paste0(formatC(mean(lbp5, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(lbp5, na.rm = TRUE), format = "e", digits = 2), ")")),
    'Baseline Hight-Corrected Total Kidney Volume (Mean(SD))' = as.character(paste0(formatC(mean(tkvht_base, na.rm = TRUE), format = "e", digits = 2), " (", formatC(sd(tkvht_base, na.rm = TRUE), format = "e", digits = 2), ")")),
  ) %>%
  pivot_longer(cols = everything(), names_to = "Characteristic", values_to = "Value") 
    

# Print the table
kable(summary_table, caption = "Table 1: Baseline Characteristics of Study Participants") %>%
  kable_styling() %>%
  row_spec(0, bold = TRUE) %>% # Bold the header
  row_spec(1:nrow(summary_table), extra_css = "border-top: 1px solid black; border-bottom: 1px solid black;")

```


```{r}
#shapiro wilkd test for normality(not great)
shapiro_results <- lapply(adpkd[, numeric_columns], shapiro.test)

shapiro_results
```

```{r}
# Get correlation matrix
numeric_columns <- sapply(adpkd, is.numeric)
cor_matrix <- cor(adpkd[, numeric_columns])   

#visualize correlation matrix
print(cor_matrix)

```


```{r}
# Get highly correlated features
high_correlations <- which(abs(cor_matrix) > 0.8, arr.ind = TRUE)

high_correlations_pairs <- data.frame(
  Column1 = rownames(cor_matrix)[high_correlations[, 1]],
  Column2 = colnames(cor_matrix)[high_correlations[, 2]],
  Correlation = cor_matrix[high_correlations]
)

# Print the result
high_correlations_pairs

#Finds that lbp4-lbp3, and lbp4-lbp1, are the only feature pairs with a correlation greater than 0.8

```


```{r}
# Address multicolinearity
# ibp4 is highly correlated with both lbp1 and lbp3, lbp4 is also the least correlated with our outcome variables so that is the feature we will delete to address the multicolinearity 

adpkd_no_multico <- adpkd[, -which(names(adpkd)== "lbp4")]
head(adpkd_no_multico)

#adpkd_no_multico is the dataset without lbp4 to address issues with multicolinearity
```


```{r}
# box plots of independent variables 
vars_to_transform <- setdiff(names(adpkd_no_multico), c("Subject.ID", "tkvht_visit2","progression","tkvht_change"))
x_variables <- adpkd_no_multico[, vars_to_transform]

for (col in names(x_variables)) {
  boxplot(x_variables[[col]], main = paste("Boxplot of", col), ylab = col)
}

# While the box plots do show that we have outliers we do not have any evidence that these outliers are an error, for now we will leave them in until we decide on the features that we are keeping and then we will revisit how to address them (possibly z-score 3 std's away)
```


```{r}
# Because the values are not normally distributed we decided to use normalization over standardization 

#normalizing x values to put them on a similar scale for analysis 
adpkd_x_vars_normalized <- as.data.frame(lapply(x_variables, function(x) {
  (x - min(x)) / (max(x) - min(x))
}))

head(adpkd_x_vars_normalized)

#normalized to a range between 0 and 1
```


Task 1: For part 2 and part 3 we should do a linear regression, a ridge regression and a lasso regression so that we can compare them and decide on which linear model works best, then we can choose the model that gives us the best AIC and lowest mse as our final model for that part 

```{r}
y_variables <- adpkd_no_multico[, c("Subject.ID", "tkvht_visit2","progression","tkvht_change")]
y_variables
```

```{r,echo=FALSE}
combined_normalized <- cbind(adpkd_x_vars_normalized, y_variables)
combined_normalized


# code to do a 70/30 split on combined normalized
set.seed(123)
#randomly split data into 70/30 
sample <- sample(c(TRUE, FALSE), nrow(combined_normalized), replace=TRUE,prob=c(0.7,0.3))
training <-combined_normalized[sample, ]
testing <- combined_normalized[!sample,]

```

```{r,echo=FALSE}
#train linear models

#linear model using tkvht_base as only predictor
model_tkv_lm <- lm(tkvht_change ~tkvht_base, data=training)
summary(model_tkv_lm)

#linear model using image featuring as predictors

#linear model: 

model_image_lm <- lm(tkvht_change ~ geom1 + geom2 + #image feature based on kidney geometry information
    gabor1 + gabor2 + gabor3 + gabor4 + gabor5 + 
    glcm1 + glcm2 + #image feature based on gray level co-occurrence matrix
    txti1 + txti2 + txti3 + txti4 + txti5 + #image feature based on image textures
    lbp1 + lbp2 + lbp3 + lbp5, # local binary pattern)
    data = training)


model_both_lm <- lm(tkvht_change ~ tkvht_base + geom1 + geom2 +  gabor1 + gabor2 + gabor3 + gabor4 + gabor5 + glcm1 + glcm2 + txti1 + txti2 + txti3 + txti4 + txti5 +  lbp1 + lbp2 + lbp3 + lbp5,  data = training)

#extract residuals for 

model_tkv_fitted<- predict(model_tkv_lm)
model_tkv_resid <-  resid(model_tkv_lm)

model_image_fitted<- predict(model_image_lm)
model_image_resid <-  resid(model_image_lm)

model_both_fitted<- predict(model_both_lm)
model_both_resid <-  resid(model_both_lm)



# check assumptions of linear regression on this model
#outliers appear to skew equal variance, normality, and linearity

#plot residuals
ggplot(data= NULL, aes(x=model_tkv_fitted,y=model_tkv_resid)) +
  geom_point()+
  geom_hline(yintercept = 0, linetype="dashed", color = "red")+
  labs(title = "figure: tkv change lm residuals vs fitted",
       x = "fitted values",
       y= "residual values")+
  theme_minimal()

#qqplot to check normality
qqnorm(model_tkv_resid, main = "Q-Q Plot of Residuals for tkv base lm")
qqline(model_tkv_resid, col= "red")


#plot residuals of lm image featuring 
ggplot(data= NULL, aes(x=model_image_fitted,y=model_image_resid)) +
  geom_point()+
  geom_hline(yintercept = 0, linetype="dashed", color = "red")+
  labs(title = "figure: model imaging lm residuals vs fitted",
       x = "fitted values",
       y= "residual values")+
  theme_minimal()

#qqplot to check normality
qqnorm(model_image_resid, main = "Q-Q Plot of Residuals for model imaging lm")
qqline(model_image_resid, col= "red")

#appears as though normality could be violated, this could be due to outliers

#plot residuals
ggplot(data= NULL, aes(x=model_both_fitted,y=model_both_resid)) +
  geom_point()+
  geom_hline(yintercept = 0, linetype="dashed", color = "red")+
  labs(title = "figure: tkv change lm residuals vs fitted",
       x = "fitted values",
       y= "residual values")+
  theme_minimal()

#qqplot to check normality
qqnorm(model_tkv_resid, main = "Q-Q Plot of Residuals for model imaging lm")
qqline(model_tkv_resid, col= "red")

```

Method 2: Lasso

```{r, echo=FALSE}

# for imaging
X_image <- as.matrix(training[, c("geom1", "geom2", "gabor1", "gabor2", "gabor3", "gabor4", "gabor5", 
                                  "glcm1", "glcm2", "txti1", "txti2", "txti3", "txti4", "txti5", "lbp1", "lbp2", "lbp3", "lbp5")])
y_image <- training$tkvht_change  # Response variable

# Fit Lasso model with a specific lambda
lasso_image <- glmnet(X_image, y_image, alpha = 1, lambda = 0.1)  # Use lambda=0.1 as an example

# Get the coefficients
lasso_image$beta

#for both: 

# Prepare the data
X_both <- as.matrix(training[, c("tkvht_base", "geom1", "geom2", "gabor1", "gabor2", "gabor3", "gabor4", "gabor5", 
                                  "glcm1", "glcm2", "txti1", "txti2", "txti3", "txti4", "txti5", "lbp1", "lbp2", "lbp3", "lbp5")])
y_both <- training$tkvht_change  # Response variable

# Fit Lasso model with a specific lambda
lasso_both <- glmnet(X_both, y_both, alpha = 1, lambda = 0.1)  # Use lambda=0.1 as an example

# Get the coefficients
lasso_both$beta

```


Ridge


```{r, echo=FALSE}


# Prepare the data for Ridge regression for image featuring
X_image <- as.matrix(training[, c("geom1", "geom2", "gabor1", "gabor2", "gabor3", "gabor4", "gabor5", 
                                  "glcm1", "glcm2", "txti1", "txti2", "txti3", "txti4", "txti5", "lbp1", "lbp2", "lbp3", "lbp5")])
y_image <- training$tkvht_change  # Response variable

# Fit Ridge model (alpha = 0 for Ridge)
ridge_image <- glmnet(X_image, y_image, alpha = 0, lambda = 0.1)  # You can choose an appropriate lambda value

# View the coefficients
ridge_image$beta


# Prepare the data for Ridge regression 
X_both <- as.matrix(training[, c("tkvht_base", "geom1", "geom2", "gabor1", "gabor2", "gabor3", "gabor4", "gabor5", 
                                  "glcm1", "glcm2", "txti1", "txti2", "txti3", "txti4", "txti5", "lbp1", "lbp2", "lbp3", "lbp5")])
y_both <- training$tkvht_change  # Response variable

# Fit Ridge model (alpha = 0 for Ridge)
ridge_both <- glmnet(X_both, y_both, alpha = 0, lambda = 0.1)  # You can choose an appropriate lambda value

# View the coefficients
ridge_both$beta


```

```{r}

# Model selection

# AIC for all models (including the two additional Linear models)
aic_tkv_lm <- AIC(model_tkv_lm)   # Linear (TKV Base)
aic_image_lm <- AIC(model_image_lm) # Linear (Image)
aic_both_lm <- AIC(model_both_lm)   # Linear (Both)

# MSE for all models (on Training Data)
# Predict and calculate MSE for Linear Models
lm_image_pred <- predict(model_image_lm, newdata = training)  # Linear (Image)
mse_lm_image <- mean((training$tkvht_change - lm_image_pred)^2)

# MSE for Linear (TKV Base)
lm_tkv_pred <- predict(model_tkv_lm, newdata = training)  # Linear (TKV Base)
mse_lm_tkv <- mean((training$tkvht_change - lm_tkv_pred)^2)

# MSE for Linear (Both)
lm_both_pred <- predict(model_both_lm, newdata = training)  # Linear (Both)
mse_lm_both <- mean((training$tkvht_change - lm_both_pred)^2)

# Lasso Model - Image
lasso_image_pred <- predict(lasso_image, s = 0.1, newx = X_image)
mse_lasso_image <- mean((training$tkvht_change - lasso_image_pred)^2)

# Ridge Model - Image
ridge_image_pred <- predict(ridge_image, s = 0.1, newx = X_image)
mse_ridge_image <- mean((training$tkvht_change - ridge_image_pred)^2)

# Lasso Model - Both
lasso_both_pred <- predict(lasso_both, s = 0.1, newx = X_both)
mse_lasso_both <- mean((training$tkvht_change - lasso_both_pred)^2)

# Ridge Model - Both
ridge_both_pred <- predict(ridge_both, s = 0.1, newx = X_both)
mse_ridge_both <- mean((training$tkvht_change - ridge_both_pred)^2)


# RMSE Calculation (Square root of MSE)
rmse_lm_tkv <- sqrt(mse_lm_tkv)
rmse_lm_image <- sqrt(mse_lm_image)
rmse_lm_both <- sqrt(mse_lm_both)

rmse_lasso_image <- sqrt(mse_lasso_image)
rmse_ridge_image <- sqrt(mse_ridge_image)
rmse_lasso_both <- sqrt(mse_lasso_both)
rmse_ridge_both <- sqrt(mse_ridge_both)

# Store AIC, MSE, and RMSE results in a data frame
results <- data.frame(
  Model = c("Linear (TKV Base)", "Linear (Image)", "Linear (Both)", 
            "Lasso (Image)", "Ridge (Image)", "Lasso (Both)", "Ridge (Both)"),
  AIC = c(aic_tkv_lm, aic_image_lm, aic_both_lm, NA, NA, NA, NA),
  MSE = c(mse_lm_tkv, mse_lm_image, mse_lm_both, 
          mse_lasso_image, mse_ridge_image, mse_lasso_both, mse_ridge_both),
  RMSE = c(rmse_lm_tkv, rmse_lm_image, rmse_lm_both, 
           rmse_lasso_image, rmse_ridge_image, rmse_lasso_both, rmse_ridge_both)
)


kable(results, caption = "Model Comparison: AIC, MSE, and RMSE", digits = 4)


```

Testing Final selection :

```{r,echo=FALSE}

# Prepare the testing data (convert to matrix, same features used for training)
X_test <- as.matrix(testing[, c("tkvht_base", "geom1", "geom2", "gabor1", "gabor2", "gabor3", "gabor4", "gabor5", 
                                "glcm1", "glcm2", "txti1", "txti2", "txti3", "txti4", "txti5", "lbp1", "lbp2", "lbp3", "lbp5")])

# Use the Ridge (Both) model to make predictions on the testing data
ridge_both_pred_test <- predict(ridge_both, s = 0.1, newx = X_test)

# Calculate MSE (Mean Squared Error) for Ridge (Both) on the testing data
mse_ridge_both_test <- mean((testing$tkvht_change - ridge_both_pred_test)^2)

# Calculate RMSE (Root Mean Squared Error)
rmse_ridge_both_test <- sqrt(mse_ridge_both_test)

# Output the results
cat("Final Evaluation Metrics for Ridge (Both) Model on Testing Data:\n")
cat("MSE (Mean Squared Error): ", round(mse_ridge_both_test, 4), "\n")
cat("RMSE (Root Mean Squared Error): ", round(rmse_ridge_both_test, 4), "\n")


# Create a data frame for the final evaluation metrics
final_metrics <- data.frame(
  Metric = c("MSE (Mean Squared Error)", "RMSE (Root Mean Squared Error)"),
  Value = c(round(mse_ridge_both_test, 4), round(rmse_ridge_both_test, 4))
)

kable(final_metrics, caption = "Final Evaluation Metrics for Ridge (Both) Model on Testing Data", digits = 4)

```


Logistic regression


```{r, echo=FALSE}

#fit logistic regression model

tkvht_base_glm <- glm(progression ~ tkvht_base, family = "binomial", data = training)

summary(tkvht_base_glm)

model_imaging_log <- glm(progression ~ geom1 + geom2 + gabor1 + gabor2 + gabor3 + gabor4 + gabor5 + glcm1 + glcm2 + txti1 + txti2 + txti3 + txti4 + txti5 + lbp1 + lbp2 + lbp3 + lbp5, data = training )

summary(model_imaging_log)

model_imaging_log <- glm(progression ~ tkvht_base +geom1 + geom2 + gabor1 + gabor2 + gabor3 + gabor4 + gabor5 + glcm1 + glcm2 + txti1 + txti2 + txti3 + txti4 + txti5 + lbp1 + lbp2 + lbp3 + lbp5, data = training )


```


Lasso and ridge

```{r, echo=FALSE}

# Prepare the data for logistic regression with Lasso regularization (L1)
X_lasso <- as.matrix(training[, c("tkvht_base", "geom1", "geom2", "gabor1", "gabor2", "gabor3", "gabor4", "gabor5", 
                                  "glcm1", "glcm2", "txti1", "txti2", "txti3", "txti4", "txti5", "lbp1", "lbp2", "lbp3", "lbp5")])
y_lasso <- training$progression  # Binary outcome variable

# Fit Lasso logistic regression (alpha = 1 for Lasso)
lasso_logit <- glmnet(X_lasso, y_lasso, family = "binomial", alpha = 1, lambda = 0.1)  # Choose a lambda value

# View the coefficients
lasso_logit$beta



# Fit Ridge logistic regression (alpha = 0 for Ridge) L2 Regularization
ridge_logit <- glmnet(X_lasso, y_lasso, family = "binomial", alpha = 0, lambda = 0.1)  # Choose a lambda value

# View the coefficients
ridge_logit$beta



```
